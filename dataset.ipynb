{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable autoreload of libraries on execution\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "\n",
    "def is_using_colab() -> bool:\n",
    "    return \"google.colab\" in str(get_ipython())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_using_colab():\n",
    "  import os\n",
    "  import zipfile\n",
    "  from google.colab import drive\n",
    "  os.system(\"pip -q install torchinfo\")\n",
    "  os.system(\"pip -q install pytorch-lightning\")\n",
    "  os.system(\"pip -q install torchmetrics[image]\")\n",
    "\n",
    "  !mkdir Experiments\n",
    "  !mkdir Datasets\n",
    "  \n",
    "  # Copy code and datasets zips\n",
    "  drive.mount('/content/drive')\n",
    "  !cp -r \"/content/drive/MyDrive/Master Thesis/02 - Code/.\" \".\"\n",
    "  # !cp -r \"/content/drive/MyDrive/Master Thesis/01 - Datasets/.\" \"Datasets/.\"\n",
    "  !cp -r \"/content/drive/MyDrive/Master Thesis/01 - Datasets/DIV2K_800_5.zip\" \"Datasets/DIV2K_800_5.zip\"\n",
    "\n",
    "  # Extract datasets zips\n",
    "  for zip in os.listdir(\"Datasets\"):\n",
    "    with zipfile.ZipFile(os.path.join(\"Datasets\", zip), 'r') as zip_ref:\n",
    "      zip_ref.extractall(os.path.join(\"Datasets\", zip.replace(\".zip\", \"\")))\n",
    "    os.remove(os.path.join(\"Datasets\", zip))\n",
    "\n",
    "  device = \"cuda\"\n",
    "  experiments_dir = \"Experiments\"\n",
    "  datasets_path = r\"Datasets/\"\n",
    "else:\n",
    "  device = \"cuda\"\n",
    "  experiments_dir = \"../05 - Experiments\"\n",
    "  datasets_path = r\"C:\\datasets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import gc\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display\n",
    "\n",
    "from media_utils import plot_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Exploration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_path = r\"G:\\VM\\Shared Folder\\bags\\0001.bag\"\n",
    "# EVENTS_TOPIC = \"/cam0/events\"\n",
    "# IMAGES_TOPIC = \"/cam0/image_raw\"\n",
    "\n",
    "dataset_path = r\"E:\\Cartelle Personali\\Fabrizio\\Universita\\Magistrale\\Tesi\\03 - Dataset\\CED_simple\\simple_jenga_destroy.bag\"\n",
    "EVENTS_TOPIC = \"/dvs/events\"\n",
    "IMAGES_TOPIC = \"/dvs/image_color\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Inspection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect messages content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_utils import inspect_bag\n",
    "inspect_bag(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Image events content:**\n",
    "\n",
    "**data**: binary image data encoded according to encoding\n",
    "\n",
    "**encoding**: format used to encode the image data - e.g. mono8, bgr8\n",
    "\n",
    "**width**, **height**: size of the image\n",
    "\n",
    "**is_bigendian**: endianness of image\n",
    "\n",
    "**step**: full row length in bytes\n",
    "\n",
    "**header**:\n",
    "\n",
    "-   **seq**: consecutively increasing ID\n",
    "-   **stamp**:\n",
    "    -   **secs**: seconds since epoch\n",
    "    -   **nsecs**: nanoseconds since stamp.secs\n",
    "-   **frame_id**: frame this data is associated with - in this dataset is always empty\n",
    "\n",
    "##### **Event events content:**\n",
    "\n",
    "**events**: array of events\n",
    "\n",
    "**width**, **height**: size of the camera sensor\n",
    "\n",
    "**header**:\n",
    "\n",
    "-   **seq**: consecutively increasing ID\n",
    "-   **stamp**:\n",
    "    -   **secs**: seconds since epoch\n",
    "    -   **nsecs**: nanoseconds since stamp.secs\n",
    "-   **frame_id**: frame this data is associated with - in this dataset is always empty\n",
    "\n",
    "##### **Events format:**\n",
    "\n",
    "**x**, **y**: coordinates of the pixel sensor\n",
    "\n",
    "**polarity**: polarity of the event in {True, False}\n",
    "\n",
    "**ts**:\n",
    "\n",
    "-   **secs**: seconds since epoch\n",
    "-   **nsecs**: nanoseconds since ts.secs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect message timestamps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_utils import inspect_message_timestamps\n",
    "inspect_message_timestamps(dataset_path, events_topic=EVENTS_TOPIC, images_topic=IMAGES_TOPIC)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The events are not completely sequential, since there can be two image events after each other (e.g. 6-7) but the first event timestamp of the next event is before the last image (e.g 8).\n",
    "\n",
    "Moreover, there are some events that are acquired after the last image has been captured.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_utils import load_bag_as_dataframes\n",
    "events_df, images_df = load_bag_as_dataframes(dataset_path, events_topic=EVENTS_TOPIC, images_topic=IMAGES_TOPIC, max_events=1e6)\n",
    "print(\"Events dataframe\")\n",
    "display(events_df)\n",
    "print(\"Images dataframe\")\n",
    "display(images_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Events and frames are already ordered by timestamp, since they are produced from a real simulation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of events per image frame\n",
    "\n",
    "Check how many events there are among each image frame.\n",
    "\n",
    "This roughly reflects the amount of movement in the sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_utils import plot_number_of_events_per_frame\n",
    "plot_number_of_events_per_frame(events_df, images_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check image frames frequency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_utils import plot_image_frames_frequency\n",
    "plot_image_frames_frequency(images_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save events for visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from media_utils import save_visual_bayer_events, save_visual_accumulated_events\n",
    "\n",
    "w, h = images_df.iloc[0].width, images_df.iloc[0].height\n",
    "folder_name = os.path.basename(dataset_path).replace(\".bag\", \"\")\n",
    "output_dir = os.path.join(os.path.dirname(dataset_path), folder_name)\n",
    "save_visual_bayer_events(events_df, w, h, os.path.join(output_dir, \"visual_events\"))\n",
    "save_visual_accumulated_events(events_df, w, h, os.path.join(output_dir, \"visual_images\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_utils import dataset_generator_from_batches\n",
    "batches_path = os.path.join(dataset_path.replace(\".bag\", \"\"), \"batches\")\n",
    "for batch in dataset_generator_from_batches(batches_path):\n",
    "\n",
    "    # Old batch format\n",
    "    (in_, events), out_ = batch\n",
    "    plot_img(in_)\n",
    "    print(events.shape)\n",
    "    plot_img(out_)\n",
    "\n",
    "    # New batch format\n",
    "    # events, img = batch\n",
    "    # print(events.shape)\n",
    "    # plot_img(img)\n",
    "    \n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets Definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PARAMS = {\n",
    "    \"limit\": None,\n",
    "    \"preload_to_RAM\": True,\n",
    "    \"crop_size\": (128, 128)\n",
    "}\n",
    "\n",
    "DATALOADER_PARAMS = {\n",
    "    \"batch_size\": 8,\n",
    "    \"num_workers\": 0,\n",
    "    \"pin_memory\": True,\n",
    "}\n",
    "\n",
    "if is_using_colab():\n",
    "    DATASET_PARAMS[\"preload_to_RAM\"] = True\n",
    "    DATALOADER_PARAMS[\"batch_size\"] = 64\n",
    "    DATALOADER_PARAMS[\"num_workers\"] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CED Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CED_DATASET_PATH = os.path.join(datasets_path, \"CEDDataset\")\n",
    "available_sequences = os.listdir(CED_DATASET_PATH)\n",
    "print(\"Available sequences:\")\n",
    "for i, seq in enumerate(available_sequences):\n",
    "    print(f\"{i:<5}{seq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import CEDDataset\n",
    "train_datasets_names = [\"simple_color_keyboard_1\", \"simple_fruit\"]\n",
    "train_dataset = CEDDataset(CED_DATASET_PATH, sequences=train_datasets_names, ignore_input_image=True, **DATASET_PARAMS)\n",
    "\n",
    "valid_datasets_names = [\"simple_rabbits\"]\n",
    "valid_dataset = CEDDataset(CED_DATASET_PATH, sequences=valid_datasets_names, ignore_input_image=True, **DATASET_PARAMS)\n",
    "\n",
    "test_datasets_names = [\"simple_color_keyboard_2\", \"simple_jenga_1\", \"simple_wires_1\"]\n",
    "test_dataset = CEDDataset(CED_DATASET_PATH, sequences=test_datasets_names, ignore_input_image=True, **DATASET_PARAMS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic Dataset from DIV2K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import DIV2KDataset\n",
    "DIV2K_DATASET_PATH = os.path.join(datasets_path, \"DIV2K_800_5\")\n",
    "train_datasets_names = [\"{:04}\".format(i) for i in range(1, 800 + 1)]\n",
    "# train_datasets_names = [\"{:04}\".format(i) for i in range(1, 25 + 1)]\n",
    "train_dataset = DIV2KDataset(DIV2K_DATASET_PATH, sequences=train_datasets_names, **DATASET_PARAMS)\n",
    "\n",
    "valid_datasets_names = [\"{:04}\".format(i) for i in range(801, 900 + 1)]\n",
    "# valid_datasets_names = [\"{:04}\".format(i) for i in range(801, 805 + 1)]\n",
    "valid_dataset = DIV2KDataset(DIV2K_DATASET_PATH, sequences=valid_datasets_names, **DATASET_PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch_from_files(paths, pre_process_fn):\n",
    "    X = []\n",
    "    y = []\n",
    "    for path in paths:\n",
    "        sample = pre_process_fn(torch.load(path))\n",
    "        X.append(torch.tensor(sample[0], dtype=torch.float32))\n",
    "        y.append(torch.tensor(sample[1], dtype=torch.float32))\n",
    "    batch = torch.stack(X), torch.stack(y)\n",
    "    return batch\n",
    "\n",
    "\n",
    "train_batch_paths = [\n",
    "    os.path.join(DIV2K_DATASET_PATH, \"0001\", \"batch_0000.pt\"),\n",
    "    os.path.join(DIV2K_DATASET_PATH, \"0002\", \"batch_0000.pt\"),\n",
    "    os.path.join(DIV2K_DATASET_PATH, \"0003\", \"batch_0000.pt\"),\n",
    "    os.path.join(DIV2K_DATASET_PATH, \"0004\", \"batch_0000.pt\"),\n",
    "    os.path.join(DIV2K_DATASET_PATH, \"0005\", \"batch_0000.pt\"),\n",
    "]\n",
    "valid_batch_paths = [\n",
    "    os.path.join(DIV2K_DATASET_PATH, \"0801\", \"batch_0000.pt\"),\n",
    "    os.path.join(DIV2K_DATASET_PATH, \"0802\", \"batch_0000.pt\"),\n",
    "    os.path.join(DIV2K_DATASET_PATH, \"0803\", \"batch_0000.pt\"),\n",
    "    os.path.join(DIV2K_DATASET_PATH, \"0804\", \"batch_0000.pt\"),\n",
    "    os.path.join(DIV2K_DATASET_PATH, \"0805\", \"batch_0000.pt\"),\n",
    "]\n",
    "\n",
    "train_batch = generate_batch_from_files(train_batch_paths, train_dataset.pre_process)\n",
    "valid_batch = generate_batch_from_files(valid_batch_paths, valid_dataset.pre_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there is no instersection between train and valid dataset\n",
    "union = set(train_datasets_names).union(set(valid_datasets_names))\n",
    "sum_of_lengths = sum([len(ds) for ds in [train_datasets_names, valid_datasets_names]])\n",
    "assert len(union) == sum_of_lengths, \"Some datasets are in common\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import CEDDataset, ConcatBatchSampler\n",
    "\n",
    "# Do not batch events coming from different datasets\n",
    "# samplers = [torch.utils.data.RandomSampler(ds) for ds in train_datasets]\n",
    "# sampler = ConcatBatchSampler(samplers=samplers, batch_size=TRAIN_DS_PARAMS[\"batch_size\"], drop_last=False)\n",
    "# train_dataloader = torch.utils.data.DataLoader(\n",
    "#     concat_ds, batch_sampler=sampler,\n",
    "# )\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, shuffle=True, **DATALOADER_PARAMS)\n",
    "valid_dataloader = torch.utils.data.DataLoader(valid_dataset, shuffle=False, **DATALOADER_PARAMS)\n",
    "\n",
    "DATASET_PARAMS.update({\n",
    "    \"train_datasets_names\": train_datasets_names, \n",
    "    \"valid_datasets_names\": valid_datasets_names, \n",
    "})\n",
    "\n",
    "print(\"Train samples: {} \\t Train batches: {:<10}\".format(len(train_dataset), len(train_dataloader)))\n",
    "print(\"Valid samples: {} \\t Valid batches: {:<10}\".format(len(valid_dataset), len(valid_dataloader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect train dataloader\n",
    "from media_utils import save_video_tensors\n",
    "frames = []\n",
    "\n",
    "# Old dataset format\n",
    "# for (i, e), o in tqdm(train_dataloader):\n",
    "\n",
    "for e, o in tqdm(train_dataloader):\n",
    "  for batch in o:\n",
    "    frames.append(batch)\n",
    "\n",
    "save_video_tensors(\"train_frames.mp4\", frames, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect valid dataloader\n",
    "from media_utils import save_video_tensors\n",
    "frames = []\n",
    "\n",
    "# Old dataset format\n",
    "# for (i, e), o in tqdm(valid_dataloader):\n",
    "\n",
    "for e, o in tqdm(valid_dataloader):\n",
    "  for batch in o:\n",
    "    frames.append(batch)\n",
    "\n",
    "save_video_tensors(\"valid_frames.mp4\", frames, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect test dataloader\n",
    "from media_utils import save_video_tensors\n",
    "frames = []\n",
    "\n",
    "# Old dataset format\n",
    "# for (i, e), o in tqdm(test_dataloader):\n",
    "\n",
    "for e, o in tqdm(test_dataloader):\n",
    "  for batch in o:\n",
    "    frames.append(batch)\n",
    "\n",
    "save_video_tensors(\"test_frames.mp4\", frames, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNet AutoEncoder Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "from models import EventsToImagesUNet\n",
    "\n",
    "PARAMS = {\n",
    "    \"input_channels\": 3 + 10,\n",
    "}\n",
    "\n",
    "model = EventsToImagesUNet(PARAMS[\"input_channels\"])\n",
    "# summary(model, input_size=(4, input_channels, 256, 336), device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import train_unet\n",
    "\n",
    "PARAMS.update({\n",
    "    \"n_epochs\": 3,\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"comment\": \"\",\n",
    "})\n",
    "PARAMS.update({\"train_dataset_params\": DATASET_PARAMS})\n",
    "\n",
    "train_unet(model, device, train_dataloader, PARAMS, log_path=experiments_dir, valid_ds=valid_dataloader, save_best_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model_thesis.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save input frames as video\n",
    "from media_utils import save_video_tensors\n",
    "imgs = []\n",
    "for batch in tqdm(train_dataloader):\n",
    "    (input_images, events_tensors), ground_truth_images = batch\n",
    "    imgs += input_images\n",
    "save_video_tensors(\"input_video.mp4\", imgs, 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save generated frames as video\n",
    "use_prev_images_as_input = False\n",
    "\n",
    "imgs = []\n",
    "for i, batch in enumerate(tqdm(train_dataloader)):\n",
    "    if use_prev_images_as_input and i != 0:\n",
    "        input_images = generated_images\n",
    "    else:\n",
    "        (input_images, events_tensors), ground_truth_images = batch\n",
    "    input_images = torch.einsum(\"bhwc -> bchw\", input_images)\n",
    "\n",
    "    input_tensors = torch.hstack((input_images, events_tensors))\n",
    "    input_tensors = input_tensors.to(device)\n",
    "\n",
    "    generated_images = model(input_tensors).cpu().detach()\n",
    "    generated_images = torch.einsum(\"bchw -> bhwc\", generated_images)\n",
    "\n",
    "    imgs += generated_images\n",
    "\n",
    "save_video_tensors(\"generated_video.mp4\", imgs, 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BS = 1; Seconds for one epoch: 142.36227083206177\n",
    "# BS = 2; Seconds for one epoch: 132.93124723434448\n",
    "# BS = 4; Seconds for one epoch: 127.95218682289124\n",
    "# BS = 8; Seconds for one epoch: 138.49955368041992\n",
    "# BS = 16; Seconds for one epoch: after 180 seconds it was at 3/10 batches so I stopped it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import TransformerModel\n",
    "\n",
    "PARAMS = {\n",
    "    \"input_shape\": (336, 256, 3),\n",
    "    \"encoding_size\": 512,\n",
    "    \"heads\": 4,\n",
    "    \"layers_number\": 2,\n",
    "}\n",
    "\n",
    "model = TransformerModel(PARAMS[\"input_shape\"], PARAMS[\"encoding_size\"], PARAMS[\"heads\"], PARAMS[\"layers_number\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import train_transformer\n",
    "\n",
    "PARAMS.update({\n",
    "    \"n_epochs\": 3,\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"comment\": \"\",\n",
    "})\n",
    "PARAMS.update({\"train_dataset_params\": DATASET_PARAMS})\n",
    "\n",
    "train_transformer(model, device, train_dataloader, PARAMS, log_path=experiments_dir, valid_ds=valid_dataloader, save_best_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "torch.save(model.state_dict(), \"transformer.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model\n",
    "model.load_state_dict(torch.load(\"transformer.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.autoencoder import EventEncoder, EventDecoder, EventAutoEncoder\n",
    "from torchinfo import summary\n",
    "\n",
    "PARAMS = {\n",
    "    \"n_filters\": [16, 16, 32, 32],\n",
    "    \"input_features\": 1,\n",
    "    \"lr\": 0.001\n",
    "}\n",
    "\n",
    "ee = EventEncoder(PARAMS[\"input_features\"], PARAMS[\"n_filters\"])\n",
    "ed = EventDecoder(PARAMS[\"n_filters\"][-1], PARAMS[\"n_filters\"][::-1])\n",
    "model = EventAutoEncoder(ee, ed, PARAMS[\"lr\"])\n",
    "\n",
    "# summary(eae, input_size=(8, 1, 128, 128), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_events = train_batch[0][:, 1]\n",
    "train_batch = (train_events.unsqueeze(1), train_events)\n",
    "valid_events = valid_batch[0][:, 1]\n",
    "valid_batch = (valid_events.unsqueeze(1), valid_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import LogImagesCallback, KerasProgressBar, ColabSaveCallback\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "PARAMS.update({\n",
    "    \"n_epochs\": 300,\n",
    "    \"comment\": \"Large - Event AE, changed training set\"\n",
    "})\n",
    "PARAMS.update({\"dataset_params\": DATASET_PARAMS})\n",
    "PARAMS.update({\"dataloader_params\": DATALOADER_PARAMS})\n",
    "\n",
    "\n",
    "callbacks = []\n",
    "callbacks.append(LogImagesCallback(train_batch, valid_batch, n=5, n_epochs=5))\n",
    "callbacks.append(KerasProgressBar())\n",
    "if is_using_colab():\n",
    "    dst_path = \"/content/drive/MyDrive/Master Thesis\"\n",
    "    colab_cb = ColabSaveCallback(\"Exp.zip\", dst_path, 60*60, [\"zip -r Exp.zip Experiments\"])\n",
    "    callbacks.append(colab_cb)\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(monitor=\"val_loss\", mode=\"min\", save_last=True)\n",
    "checkpoint_callback.CHECKPOINT_NAME_LAST = \"{epoch}-last\"\n",
    "callbacks.append(checkpoint_callback)\n",
    "\n",
    "logger = pl.loggers.TensorBoardLogger(experiments_dir, version=PARAMS[\"comment\"])\n",
    "log_every = 50\n",
    "if len(train_dataloader) < log_every:\n",
    "    log_every = 1\n",
    "\n",
    "params_markdown = \"```json\\n\" + json.dumps(PARAMS, indent=2).replace(\"\\n\",\"  \\n\") + \"\\n```\"\n",
    "logger.experiment.add_text(\"params\", params_markdown)\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=PARAMS[\"n_epochs\"], callbacks=callbacks, accelerator=\"gpu\", profiler=None, logger=logger, log_every_n_steps=log_every)\n",
    "trainer.fit(model, train_dataloader, valid_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Event Encoder Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import EventEncoderTransformer\n",
    "from torchinfo import summary\n",
    "\n",
    "PARAMS = {\n",
    "    \"output_shape\": (336, 256, 3),\n",
    "    \"encoding_size\": 336,\n",
    "    \"heads\": 4,\n",
    "    \"layers_number\": 6\n",
    "}\n",
    "\n",
    "# The model is going to use the previous AutoEncoder, so be sure to use a pre-trained version of it\n",
    "\n",
    "model = EventEncoderTransformer(\n",
    "    output_shape=PARAMS[\"output_shape\"], \n",
    "    encoder=eae.encoder, \n",
    "    encoding_size=PARAMS[\"encoding_size\"], \n",
    "    heads=PARAMS[\"heads\"], \n",
    "    layers_number=PARAMS[\"layers_number\"]\n",
    ")\n",
    "\n",
    "# summary(model, input_size=(4, 10, 256, 336))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import train_transformer\n",
    "\n",
    "PARAMS.update({\n",
    "    \"n_epochs\": 300,\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"comment\": \"\"\n",
    "})\n",
    "PARAMS.update({\"train_dataset_params\": DATASET_PARAMS})\n",
    "\n",
    "train_transformer(model, device, train_dataloader, PARAMS, log_path=experiments_dir, valid_ds=valid_dataloader, save_best_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "torch.save(model.state_dict(), \"model_tae.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model.load_state_dict(torch.load(\"model_tae.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ViT-like Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean VRAM\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from models.transformer import VisionTransformer\n",
    "\n",
    "PARAMS = {\n",
    "    \"input_shape\": (10, 128, 128),\n",
    "    \"patch_size\": (32, 32),\n",
    "    \"encoding_size\": 256,\n",
    "    \"heads\": 4,\n",
    "    \"layers_number\": 3,\n",
    "    \"use_linear_proj\": True,\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"use_LPIPS\": False,\n",
    "    \"vgg_layer\": \"features.35\",\n",
    "}\n",
    "model = VisionTransformer(**PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_using_colab():\n",
    "    from IPython import get_ipython\n",
    "    get_ipython().magic('tensorboard --logdir \"Experiments/lightning_logs\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import LogImagesCallback, KerasProgressBar\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "PARAMS.update({\n",
    "    \"n_epochs\": 2,\n",
    "    \"comment\": \"Test with VGG54\"\n",
    "})\n",
    "PARAMS.update({\"train_dataset_params\": DATASET_PARAMS})\n",
    "\n",
    "callbacks = []\n",
    "\n",
    "train_batch = next(iter(train_dataloader))\n",
    "valid_batch = next(iter(valid_dataloader))\n",
    "callbacks.append(LogImagesCallback(train_batch, valid_batch, n=5, n_epochs=5))\n",
    "\n",
    "callbacks.append(KerasProgressBar())\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(monitor=\"val_loss\", mode=\"min\", save_last=True)\n",
    "checkpoint_callback.CHECKPOINT_NAME_LAST = \"{epoch}-last\"\n",
    "callbacks.append(checkpoint_callback)\n",
    "\n",
    "logger = pl.loggers.TensorBoardLogger(experiments_dir, version=PARAMS[\"comment\"])\n",
    "log_every = 50\n",
    "if len(train_dataloader) < log_every:\n",
    "    log_every = 1\n",
    "\n",
    "params_markdown = \"```json\\n\" + json.dumps(PARAMS, indent=2).replace(\"\\n\",\"  \\n\") + \"\\n```\"\n",
    "logger.experiment.add_text(\"params\", params_markdown)\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=PARAMS[\"n_epochs\"], callbacks=callbacks, accelerator=\"gpu\", profiler=None, logger=logger, log_every_n_steps=log_every)\n",
    "trainer.fit(model, train_dataloader, valid_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "torch.save(model.state_dict(), \"model_vit.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model.load_state_dict(torch.load(\"model_vit.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conv ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean VRAM\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from models.transformer import VisionTransformerConv\n",
    "\n",
    "PARAMS = {\n",
    "    \"input_shape\": (10, 128, 128),\n",
    "    \"patch_size\": (32, 32),\n",
    "    \"heads\": 4,\n",
    "    \"layers_number\": 1,\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"image_loss_weight\": 1,\n",
    "    \"feature_loss_weight\": 1e-2,\n",
    "}\n",
    "model = VisionTransformerConv(**PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "summary(model, input_size=(8, 10, 128, 128), col_names=[\"input_size\", \"output_size\", \"num_params\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import LogImagesCallback, KerasProgressBar, ColabSaveCallback\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "PARAMS.update({\n",
    "    \"n_epochs\": 1000,\n",
    "    \"comment\": \"Large, Long - 1 il, 1e-2 fl, bn relu, maxpool\"\n",
    "})\n",
    "PARAMS.update({\"dataset_params\": DATASET_PARAMS})\n",
    "PARAMS.update({\"dataloader_params\": DATALOADER_PARAMS})\n",
    "\n",
    "\n",
    "callbacks = []\n",
    "callbacks.append(LogImagesCallback(train_batch, valid_batch, n=5, n_epochs=5))\n",
    "callbacks.append(KerasProgressBar())\n",
    "if is_using_colab():\n",
    "    dst_path = \"/content/drive/MyDrive/Master Thesis\"\n",
    "    colab_cb = ColabSaveCallback(\"Exp.zip\", dst_path, 60*60, [\"zip -r Exp.zip Experiments\"])\n",
    "    callbacks.append(colab_cb)\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(monitor=\"val_loss\", mode=\"min\", save_last=True)\n",
    "checkpoint_callback.CHECKPOINT_NAME_LAST = \"{epoch}-last\"\n",
    "callbacks.append(checkpoint_callback)\n",
    "\n",
    "logger = pl.loggers.TensorBoardLogger(experiments_dir, version=PARAMS[\"comment\"])\n",
    "log_every = 50\n",
    "if len(train_dataloader) < log_every:\n",
    "    log_every = 1\n",
    "\n",
    "params_markdown = \"```json\\n\" + json.dumps(PARAMS, indent=2).replace(\"\\n\",\"  \\n\") + \"\\n```\"\n",
    "logger.experiment.add_text(\"params\", params_markdown)\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=PARAMS[\"n_epochs\"], callbacks=callbacks, accelerator=\"gpu\", profiler=None, logger=logger, log_every_n_steps=log_every)\n",
    "trainer.fit(model, train_dataloader, valid_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip -r Exp.zip \"Experiments\"\n",
    "!mv \"Exp.zip\"  \"/content/drive/MyDrive/Master Thesis\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.transformer import VisionTransformerConv\n",
    "checkpoint_path = r\"E:\\Cartelle Personali\\Fabrizio\\Universita\\Magistrale\\Tesi\\05 - Experiments\\lightning_logs\\1e-2 FL Long training\\checkpoints\\epoch=465-step=115102.ckpt\"\n",
    "model = VisionTransformerConv.load_from_checkpoint(checkpoint_path, feature_loss_weight=None, map_location=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import LogImagesCallback\n",
    "\n",
    "def fig_to_numpy(fig):\n",
    "    canvas = fig.canvas \n",
    "    canvas.draw()\n",
    "    rgba = np.asarray(canvas.buffer_rgba())\n",
    "    return rgba\n",
    "\n",
    "def create_figure(batch, model):\n",
    "    cb = LogImagesCallback(train_batch, valid_batch)\n",
    "    x, y = batch\n",
    "    outputs = model(x)[0]\n",
    "    figs = []\n",
    "    for i in range(len(x)):\n",
    "        fig = cb._create_plot(outputs[i], y[i])\n",
    "        figs.append(fig_to_numpy(fig))\n",
    "        plt.close(fig)\n",
    "    return np.hstack(figs)\n",
    "\n",
    "path_split = checkpoint_path.split(os.sep)\n",
    "RUN_NAME = path_split[path_split.index(\"checkpoints\") - 1]\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.axis(\"off\")\n",
    "train_fig = create_figure(train_batch, model)\n",
    "plt.title(f\"{RUN_NAME} | Train\")\n",
    "plt.imsave(f\"{RUN_NAME}_train.png\", train_fig)\n",
    "plt.imshow(train_fig)\n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.axis(\"off\")\n",
    "plt.title(f\"{RUN_NAME} | Valid\")\n",
    "valid_fig = create_figure(valid_batch, model)\n",
    "plt.imsave(f\"{RUN_NAME}_valid.png\", valid_fig)\n",
    "plt.imshow(valid_fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save video results on train and test set\n",
    "from media_utils import save_predicted_video\n",
    "save_predicted_video(model, device, train_dataloader, \"train_prediction.mp4\")\n",
    "save_predicted_video(model, device, test_dataloader, \"test_prediction.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from media_utils import save_events_frames_visualization\n",
    "sensor_size = (128, 128)\n",
    "save_events_frames_visualization(sensor_size, \"train_inspect.mp4\", train_dataloader, model=model, fps=15)\n",
    "save_events_frames_visualization(sensor_size, \"test_inspect.mp4\", test_dataloader, model=model, fps=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from media_utils import plot_square, predict_n_images\n",
    "\n",
    "N_EVAL_IMAGES = 16\n",
    "\n",
    "results = predict_n_images(train_dataset, N_EVAL_IMAGES, model)\n",
    "plot_square(results, size=3)\n",
    "\n",
    "results = predict_n_images(test_dataset, N_EVAL_IMAGES, model)\n",
    "plot_square(results, size=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metadata parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "metadata_path = r\"E:\\Cartelle Personali\\Fabrizio\\Universita\\Magistrale\\Tesi\\05 - Experiments\\2022-08-31 15-36-51\\metadata.json\"\n",
    "with open(metadata_path, encoding=\"utf8\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Number of epochs:\", len(data[\"epochs\"]))\n",
    "train_loss = [epoch[\"train_loss\"] for epoch in data[\"epochs\"]]\n",
    "valid_loss = [epoch[\"valid_loss\"] for epoch in data[\"epochs\"]]\n",
    "plt.plot(range(len(data[\"epochs\"])), train_loss, color=\"blue\", label=\"Training Loss\")\n",
    "plt.plot(range(len(data[\"epochs\"])), valid_loss, color=\"orange\", label=\"Valid loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_utils import dataset_generator_from_bag, save_events_frames_view\n",
    "# path = r\"G:\\CED Datasets\\simple_color_keyboard_2.bag\"\n",
    "# gen = dataset_generator_from_bag(path, \"/dvs/events\", \"/dvs/image_color\", min_n_events=10000, crop_size=(128, 128))\n",
    "path = r\"G:\\VM\\Shared Folder\\bags\\DIV2K_0.5\\0804.bag\"\n",
    "gen = dataset_generator_from_bag(path, \"/cam0/events\", \"/cam0/image_raw\", crop_size=(128, 128))\n",
    "save_events_frames_view(\"signore.mp4\", gen, model=model, denorm=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAM Image Callback Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "\n",
    "from utils import KerasProgressBar, LogImagesCallback\n",
    "\n",
    "class UselessModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.param = torch.nn.parameter.Parameter(torch.tensor(0, dtype=torch.float32))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.param\n",
    "\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        return np.random.rand() - self.param\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        return np.random.rand() - self.param\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "model = UselessModel()\n",
    "\n",
    "callbacks = []\n",
    "train_batch = next(iter(train_dataloader))\n",
    "valid_batch = next(iter(valid_dataloader))\n",
    "callbacks.append(LogImagesCallback(train_batch, valid_batch, n=5, n_epochs=1))\n",
    "callbacks.append(KerasProgressBar())\n",
    "\n",
    "logger = pl.loggers.TensorBoardLogger(experiments_dir, \"test CB\")\n",
    "log_every = 50\n",
    "if len(train_dataloader) < log_every:\n",
    "    log_every = 1\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=500, callbacks=callbacks, accelerator=\"gpu\", logger=logger, log_every_n_steps=log_every, enable_progress_bar=True)\n",
    "trainer.fit(model, train_dataloader, valid_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LPIPS Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import vgg19, VGG19_Weights\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from torch.nn.functional import mse_loss\n",
    "from torchvision.models.feature_extraction import get_graph_node_names, create_feature_extractor\n",
    "import torch\n",
    "\n",
    "features_outputs = []\n",
    "\n",
    "img_path = r\"E:\\Cartelle Personali\\Fabrizio\\Universita\\Magistrale\\Tesi\\03 - Dataset\\CED_simple\\simple_color_keyboard_1\\color_images\\0.png\"\n",
    "img2_path = r\"E:\\Cartelle Personali\\Fabrizio\\Universita\\Magistrale\\Tesi\\03 - Dataset\\CED_simple\\simple_color_keyboard_1\\color_images\\130.png\"\n",
    "img3_path = r\"E:\\Cartelle Personali\\Fabrizio\\Universita\\Magistrale\\Tesi\\03 - Dataset\\CED_simple\\simple_fruit\\color_images\\0.png\"\n",
    "img = plt.imread(img_path)[:,:,:3]\n",
    "img_noise = img * np.random.rand(*img.shape)\n",
    "img_blur = gaussian_filter(img, sigma=(5, 5, 0))\n",
    "img2 = plt.imread(img2_path)[:,:,:3]\n",
    "img3 = plt.imread(img3_path)[:,:,:3]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(np.hstack((img, img_noise, img_blur, img2, img3)))\n",
    "plt.show()\n",
    "\n",
    "img = torch.Tensor(img).permute(2, 0, 1).unsqueeze(0)\n",
    "img_noise = torch.Tensor(img_noise).permute(2, 0, 1).unsqueeze(0)\n",
    "img_blur = torch.Tensor(img_blur).permute(2, 0, 1).unsqueeze(0)\n",
    "img2 = torch.Tensor(img2).permute(2, 0, 1).unsqueeze(0)\n",
    "img3 = torch.Tensor(img3).permute(2, 0, 1).unsqueeze(0)\n",
    "\n",
    "vgg_weights = VGG19_Weights.IMAGENET1K_V1\n",
    "vgg = vgg19(weights=vgg_weights)\n",
    "vgg.eval()\n",
    "vgg_preprocess = vgg_weights.transforms()\n",
    "\n",
    "# train_nodes, eval_nodes = get_graph_node_names(vgg)\n",
    "# print(train_nodes)\n",
    "\n",
    "VGG_LAYER = \"features.35\"\n",
    "vgg_extractor = create_feature_extractor(vgg, [VGG_LAYER])\n",
    "\n",
    "features_img = vgg_extractor(vgg_preprocess(img))[VGG_LAYER]\n",
    "features_noise = vgg_extractor(vgg_preprocess(img_noise))[VGG_LAYER]\n",
    "features_blur = vgg_extractor(vgg_preprocess(img_blur))[VGG_LAYER]\n",
    "features_img2 = vgg_extractor(vgg_preprocess(img2))[VGG_LAYER]\n",
    "features_img3 = vgg_extractor(vgg_preprocess(img3))[VGG_LAYER]\n",
    "\n",
    "mse_noise = mse_loss(img, img_noise)\n",
    "mse_blur = mse_loss(img, img_blur)\n",
    "mse_same = mse_loss(img, img)\n",
    "mse_img2 = mse_loss(img, img2)\n",
    "mse_img3 = mse_loss(img, img3)\n",
    "\n",
    "print(f\"{'MSE:':10}  same - {mse_same:.4f}, noise - {mse_noise:.4f}, blur - {mse_blur:.4f}, img2 - {mse_img2:.4f}, img3 - {mse_img3:.4f}\")\n",
    "\n",
    "mse_features_noise = mse_loss(features_img, features_noise)\n",
    "mse_features_blur = mse_loss(features_img, features_blur)\n",
    "mse_features_same = mse_loss(features_img, features_img)\n",
    "mse_features_img2 = mse_loss(features_img, features_img2)\n",
    "mse_features_img3 = mse_loss(features_img, features_img3)\n",
    "\n",
    "print(f\"{'LPIPS:':10}  same - {mse_features_same:.4f}, noise - {mse_features_noise:.4f}, blur - {mse_features_blur:.4f}, img2 - {mse_features_img2:.4f}, img3 - {mse_features_img3:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary File Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_utils import dataset_generator_from_bag, dataset_generator_from_binary, save_samples_to_disk\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "bag_file_path = r\"G:\\VM\\Shared Folder\\bags\\COCO\\000000000139.bag\"\n",
    "bin_file_path = r\"G:\\VM\\Shared Folder\\bags\\COCO\\000000000139.bin\"\n",
    "\n",
    "\n",
    "bag_gen = dataset_generator_from_bag(bag_file_path, events_topic=\"/cam0/events\", image_topic=\"/cam0/image_raw\")\n",
    "save_samples_to_disk(bag_gen, \"test_bag_batches\", False)\n",
    "bag_gen = dataset_generator_from_bag(bag_file_path, events_topic=\"/cam0/events\", image_topic=\"/cam0/image_raw\")\n",
    "save_samples_to_disk(bag_gen, \"test_bag_batches_compressed\", True)\n",
    "bin_gen = dataset_generator_from_binary(bin_file_path)\n",
    "save_samples_to_disk(bin_gen, \"test_bin_batches\", False)\n",
    "bin_gen = dataset_generator_from_binary(bin_file_path)\n",
    "save_samples_to_disk(bin_gen, \"test_bin_batches_compressed\", True)\n",
    "\n",
    "for i, (bagf, binf) in enumerate(zip(bag_gen, bin_gen)):\n",
    "    events_equal = np.array_equal(bagf[0], binf[0])\n",
    "    img_equal = np.array_equal(bagf[1], binf[1])\n",
    "    print(events_equal, img_equal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colors channeling (scrapped for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "batch = 2\n",
    "bins = 2\n",
    "w = 16\n",
    "h = 16\n",
    "\n",
    "x = torch.arange(0, batch*bins*w*h) + 1\n",
    "x = x.reshape(batch, bins, h, w)\n",
    "\n",
    "# x = x.reshape(batch, bins, h // 4, w // 4, 2, 2, 2, 2)\n",
    "\n",
    "r = x[:, :, ::2, ::2]\n",
    "g = x[:, :, ::2, 1::2]\n",
    "G = x[:, :, 1::2, ::2]\n",
    "b = x[:, :, 1::2, 1::2]\n",
    "x = torch.stack((r,g,G,b), dim=2).squeeze()\n",
    "print(x.shape, x)\n",
    "\n",
    "# x = x.transpose(4, 5)\n",
    "# x = x.transpose(5, 7)\n",
    "\n",
    "# x = x.reshape(batch, bins, 4, h // 2, w // 2)\n",
    "# x = x.permute(0, 1, 5, 3, 2, 4)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3.10.5 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f60749a70c461b41520daeabcfcd14a2d1bc9b0f5ff8acdc44832defeb641db7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
