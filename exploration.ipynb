{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable autoreload of libraries on execution\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%load_ext tensorboard\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Exploration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = r\"G:\\VM\\Shared Folder\\bags\\0001.bag\"\n",
    "EVENTS_TOPIC = \"/cam0/events\"\n",
    "IMAGES_TOPIC = \"/cam0/image_raw\"\n",
    "\n",
    "# dataset_path = r\"E:\\Cartelle Personali\\Fabrizio\\Universita\\Magistrale\\Tesi\\03 - Dataset\\CED_simple\\simple_jenga_destroy.bag\"\n",
    "# EVENTS_TOPIC = \"/dvs/events\"\n",
    "# IMAGES_TOPIC = \"/dvs/image_color\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Inspection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect messages content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_utils import inspect_bag\n",
    "inspect_bag(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Image events content:**\n",
    "\n",
    "**data**: binary image data encoded according to encoding\n",
    "\n",
    "**encoding**: format used to encode the image data - e.g. mono8, bgr8\n",
    "\n",
    "**width**, **height**: size of the image\n",
    "\n",
    "**is_bigendian**: endianness of image\n",
    "\n",
    "**step**: full row length in bytes\n",
    "\n",
    "**header**:\n",
    "\n",
    "-   **seq**: consecutively increasing ID\n",
    "-   **stamp**:\n",
    "    -   **secs**: seconds since epoch\n",
    "    -   **nsecs**: nanoseconds since stamp.secs\n",
    "-   **frame_id**: frame this data is associated with - in this dataset is always empty\n",
    "\n",
    "##### **Event events content:**\n",
    "\n",
    "**events**: array of events\n",
    "\n",
    "**width**, **height**: size of the camera sensor\n",
    "\n",
    "**header**:\n",
    "\n",
    "-   **seq**: consecutively increasing ID\n",
    "-   **stamp**:\n",
    "    -   **secs**: seconds since epoch\n",
    "    -   **nsecs**: nanoseconds since stamp.secs\n",
    "-   **frame_id**: frame this data is associated with - in this dataset is always empty\n",
    "\n",
    "##### **Events format:**\n",
    "\n",
    "**x**, **y**: coordinates of the pixel sensor\n",
    "\n",
    "**polarity**: polarity of the event in {True, False}\n",
    "\n",
    "**ts**:\n",
    "\n",
    "-   **secs**: seconds since epoch\n",
    "-   **nsecs**: nanoseconds since ts.secs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect message timestamps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_utils import inspect_message_timestamps\n",
    "inspect_message_timestamps(dataset_path, events_topic=EVENTS_TOPIC, images_topic=IMAGES_TOPIC)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The events are not completely sequential, since there can be two image events after each other (e.g. 6-7) but the first event timestamp of the next event is before the last image (e.g 8).\n",
    "\n",
    "Moreover, there are some events that are acquired after the last image has been captured.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_utils import load_bag_as_dataframes\n",
    "events_df, images_df = load_bag_as_dataframes(dataset_path, events_topic=EVENTS_TOPIC, images_topic=IMAGES_TOPIC, max_events=1e6)\n",
    "print(\"Events dataframe\")\n",
    "display(events_df)\n",
    "print(\"Images dataframe\")\n",
    "display(images_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Events and frames are already ordered by timestamp, since they are produced from a real simulation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of events per image frame\n",
    "\n",
    "Check how many events there are among each image frame.\n",
    "\n",
    "This roughly reflects the amount of movement in the sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_utils import plot_number_of_events_per_frame\n",
    "plot_number_of_events_per_frame(events_df, images_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check image frames frequency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_utils import plot_image_frames_frequency\n",
    "plot_image_frames_frequency(images_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save events for visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from media_utils import save_visual_bayer_events, save_visual_accumulated_events\n",
    "\n",
    "w, h = images_df.iloc[0].width, images_df.iloc[0].height\n",
    "folder_name = os.path.basename(dataset_path).replace(\".bag\", \"\")\n",
    "output_dir = os.path.join(os.path.dirname(dataset_path), folder_name)\n",
    "save_visual_bayer_events(events_df, w, h, os.path.join(output_dir, \"visual_events\"))\n",
    "save_visual_accumulated_events(events_df, w, h, os.path.join(output_dir, \"visual_images\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metadata parsing (old logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "metadata_path = r\"E:\\Cartelle Personali\\Fabrizio\\Universita\\Magistrale\\Tesi\\05 - Experiments\\2022-08-31 15-36-51\\metadata.json\"\n",
    "with open(metadata_path, encoding=\"utf8\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Number of epochs:\", len(data[\"epochs\"]))\n",
    "train_loss = [epoch[\"train_loss\"] for epoch in data[\"epochs\"]]\n",
    "valid_loss = [epoch[\"valid_loss\"] for epoch in data[\"epochs\"]]\n",
    "plt.plot(range(len(data[\"epochs\"])), train_loss, color=\"blue\", label=\"Training Loss\")\n",
    "plt.plot(range(len(data[\"epochs\"])), valid_loss, color=\"orange\", label=\"Valid loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_utils import dataset_generator_from_bag, save_events_frames_view\n",
    "# path = r\"G:\\CED Datasets\\simple_color_keyboard_2.bag\"\n",
    "# gen = dataset_generator_from_bag(path, \"/dvs/events\", \"/dvs/image_color\", min_n_events=10000, crop_size=(128, 128))\n",
    "path = r\"G:\\VM\\Shared Folder\\bags\\DIV2K_0.5\\0804.bag\"\n",
    "gen = dataset_generator_from_bag(path, \"/cam0/events\", \"/cam0/image_raw\", crop_size=(128, 128))\n",
    "save_events_frames_view(\"signore.mp4\", gen, model=model, denorm=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAM Image Callback Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "\n",
    "from utils import KerasProgressBar, LogImagesCallback\n",
    "\n",
    "class UselessModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.param = torch.nn.parameter.Parameter(torch.tensor(0, dtype=torch.float32))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.param\n",
    "\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        return np.random.rand() - self.param\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        return np.random.rand() - self.param\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "model = UselessModel()\n",
    "\n",
    "callbacks = []\n",
    "train_batch = next(iter(train_dataloader))\n",
    "valid_batch = next(iter(valid_dataloader))\n",
    "callbacks.append(LogImagesCallback(train_batch, valid_batch, n=5, n_epochs=1))\n",
    "callbacks.append(KerasProgressBar())\n",
    "\n",
    "logger = pl.loggers.TensorBoardLogger(experiments_dir, \"test CB\")\n",
    "log_every = 50\n",
    "if len(train_dataloader) < log_every:\n",
    "    log_every = 1\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=500, callbacks=callbacks, accelerator=\"gpu\", logger=logger, log_every_n_steps=log_every, enable_progress_bar=True)\n",
    "trainer.fit(model, train_dataloader, valid_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LPIPS Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import vgg19, VGG19_Weights\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from torch.nn.functional import mse_loss\n",
    "from torchvision.models.feature_extraction import get_graph_node_names, create_feature_extractor\n",
    "import torch\n",
    "\n",
    "features_outputs = []\n",
    "\n",
    "img_path = r\"E:\\Cartelle Personali\\Fabrizio\\Universita\\Magistrale\\Tesi\\03 - Dataset\\CED_simple\\simple_color_keyboard_1\\color_images\\0.png\"\n",
    "img2_path = r\"E:\\Cartelle Personali\\Fabrizio\\Universita\\Magistrale\\Tesi\\03 - Dataset\\CED_simple\\simple_color_keyboard_1\\color_images\\130.png\"\n",
    "img3_path = r\"E:\\Cartelle Personali\\Fabrizio\\Universita\\Magistrale\\Tesi\\03 - Dataset\\CED_simple\\simple_fruit\\color_images\\0.png\"\n",
    "img = plt.imread(img_path)[:,:,:3]\n",
    "img_noise = img * np.random.rand(*img.shape)\n",
    "img_blur = gaussian_filter(img, sigma=(5, 5, 0))\n",
    "img2 = plt.imread(img2_path)[:,:,:3]\n",
    "img3 = plt.imread(img3_path)[:,:,:3]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(np.hstack((img, img_noise, img_blur, img2, img3)))\n",
    "plt.show()\n",
    "\n",
    "img = torch.Tensor(img).permute(2, 0, 1).unsqueeze(0)\n",
    "img_noise = torch.Tensor(img_noise).permute(2, 0, 1).unsqueeze(0)\n",
    "img_blur = torch.Tensor(img_blur).permute(2, 0, 1).unsqueeze(0)\n",
    "img2 = torch.Tensor(img2).permute(2, 0, 1).unsqueeze(0)\n",
    "img3 = torch.Tensor(img3).permute(2, 0, 1).unsqueeze(0)\n",
    "\n",
    "vgg_weights = VGG19_Weights.IMAGENET1K_V1\n",
    "vgg = vgg19(weights=vgg_weights)\n",
    "vgg.eval()\n",
    "vgg_preprocess = vgg_weights.transforms()\n",
    "\n",
    "# train_nodes, eval_nodes = get_graph_node_names(vgg)\n",
    "# print(train_nodes)\n",
    "\n",
    "VGG_LAYER = \"features.35\"\n",
    "vgg_extractor = create_feature_extractor(vgg, [VGG_LAYER])\n",
    "\n",
    "features_img = vgg_extractor(vgg_preprocess(img))[VGG_LAYER]\n",
    "features_noise = vgg_extractor(vgg_preprocess(img_noise))[VGG_LAYER]\n",
    "features_blur = vgg_extractor(vgg_preprocess(img_blur))[VGG_LAYER]\n",
    "features_img2 = vgg_extractor(vgg_preprocess(img2))[VGG_LAYER]\n",
    "features_img3 = vgg_extractor(vgg_preprocess(img3))[VGG_LAYER]\n",
    "\n",
    "mse_noise = mse_loss(img, img_noise)\n",
    "mse_blur = mse_loss(img, img_blur)\n",
    "mse_same = mse_loss(img, img)\n",
    "mse_img2 = mse_loss(img, img2)\n",
    "mse_img3 = mse_loss(img, img3)\n",
    "\n",
    "print(f\"{'MSE:':10}  same - {mse_same:.4f}, noise - {mse_noise:.4f}, blur - {mse_blur:.4f}, img2 - {mse_img2:.4f}, img3 - {mse_img3:.4f}\")\n",
    "\n",
    "mse_features_noise = mse_loss(features_img, features_noise)\n",
    "mse_features_blur = mse_loss(features_img, features_blur)\n",
    "mse_features_same = mse_loss(features_img, features_img)\n",
    "mse_features_img2 = mse_loss(features_img, features_img2)\n",
    "mse_features_img3 = mse_loss(features_img, features_img3)\n",
    "\n",
    "print(f\"{'LPIPS:':10}  same - {mse_features_same:.4f}, noise - {mse_features_noise:.4f}, blur - {mse_features_blur:.4f}, img2 - {mse_features_img2:.4f}, img3 - {mse_features_img3:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary File Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_utils import dataset_generator_from_bag, dataset_generator_from_binary, save_samples_to_disk\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "bag_file_path = r\"G:\\VM\\Shared Folder\\bags\\COCO\\000000000139.bag\"\n",
    "bin_file_path = r\"G:\\VM\\Shared Folder\\bags\\COCO\\000000000139.bin\"\n",
    "\n",
    "\n",
    "bag_gen = dataset_generator_from_bag(bag_file_path, events_topic=\"/cam0/events\", image_topic=\"/cam0/image_raw\")\n",
    "save_samples_to_disk(bag_gen, \"test_bag_batches\", False)\n",
    "bag_gen = dataset_generator_from_bag(bag_file_path, events_topic=\"/cam0/events\", image_topic=\"/cam0/image_raw\")\n",
    "save_samples_to_disk(bag_gen, \"test_bag_batches_compressed\", True)\n",
    "bin_gen = dataset_generator_from_binary(bin_file_path)\n",
    "save_samples_to_disk(bin_gen, \"test_bin_batches\", False)\n",
    "bin_gen = dataset_generator_from_binary(bin_file_path)\n",
    "save_samples_to_disk(bin_gen, \"test_bin_batches_compressed\", True)\n",
    "\n",
    "for i, (bagf, binf) in enumerate(zip(bag_gen, bin_gen)):\n",
    "    events_equal = np.array_equal(bagf[0], binf[0])\n",
    "    img_equal = np.array_equal(bagf[1], binf[1])\n",
    "    print(events_equal, img_equal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colors channeling (scrapped for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "batch = 2\n",
    "bins = 2\n",
    "w = 16\n",
    "h = 16\n",
    "\n",
    "x = torch.arange(0, batch*bins*w*h) + 1\n",
    "x = x.reshape(batch, bins, h, w)\n",
    "\n",
    "# x = x.reshape(batch, bins, h // 4, w // 4, 2, 2, 2, 2)\n",
    "\n",
    "r = x[:, :, ::2, ::2]\n",
    "g = x[:, :, ::2, 1::2]\n",
    "G = x[:, :, 1::2, ::2]\n",
    "b = x[:, :, 1::2, 1::2]\n",
    "x = torch.stack((r,g,G,b), dim=2).squeeze()\n",
    "print(x.shape, x)\n",
    "\n",
    "# x = x.transpose(4, 5)\n",
    "# x = x.transpose(5, 7)\n",
    "\n",
    "# x = x.reshape(batch, bins, 4, h // 2, w // 2)\n",
    "# x = x.permute(0, 1, 5, 3, 2, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset events values range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.11908084 0.118364096\n"
     ]
    }
   ],
   "source": [
    "from dataset import DIV2KDataset\n",
    "\n",
    "DIV2K_DATASET_PATH = r\"C:\\datasets\\DIV2K_5_fix\"\n",
    "datasets = [\"{:04}\".format(i) for i in range(1, 900 + 1)]\n",
    "\n",
    "mins = []\n",
    "maxs = []\n",
    "for dataset in datasets:\n",
    "    d = DIV2KDataset(DIV2K_DATASET_PATH, sequences=[dataset], crop_size=(128, 128), preload_to_RAM=True, normalize_events=True)\n",
    "    v_min, v_max = None, None\n",
    "    for events, img in d:\n",
    "        d_min, d_max = events.min(), events.max()\n",
    "        if v_min is None or d_min < v_min:\n",
    "            v_min = d_min\n",
    "        if v_max is None or d_max > v_max:\n",
    "            v_max = d_max\n",
    "    mins.append(d_min)\n",
    "    maxs.append(d_max)\n",
    "\n",
    "print(min(mins), max(maxs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f60749a70c461b41520daeabcfcd14a2d1bc9b0f5ff8acdc44832defeb641db7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
