{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable autoreload of libraries on execution\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "\n",
    "# Set this to True when you just want to test if a model works, without proper training\n",
    "TEST_MODE = False \n",
    "\n",
    "def is_using_colab() -> bool:\n",
    "    \"\"\"Return if running on the Colab platform.\"\"\"\n",
    "    return \"google.colab\" in str(get_ipython())\n",
    "\n",
    "if is_using_colab():\n",
    "    import os\n",
    "    import zipfile\n",
    "    from google.colab import drive\n",
    "\n",
    "    if TEST_MODE:\n",
    "        print(\"⚠️ Running on TEST_MODE, if that's what you wanted to do, ignore this. ⚠️\")\n",
    "\n",
    "    # Install required libraries\n",
    "    os.system(\"pip -q install torchinfo\")\n",
    "    os.system(\"pip -q install pytorch-lightning\")\n",
    "    os.system(\"pip -q install torchmetrics[image]\")\n",
    "    os.system(\"pip -q install bagpy\")\n",
    "    \n",
    "    # Create folders\n",
    "    !mkdir Experiments\n",
    "    !mkdir Datasets\n",
    "    \n",
    "    # Copy code and datasets zips from Drive\n",
    "    drive.mount('/content/drive')\n",
    "    !cp -r \"/content/drive/MyDrive/Master Thesis/02 - Code/.\" \".\"\n",
    "    # !cp -r \"/content/drive/MyDrive/Master Thesis/01 - Datasets/.\" \"Datasets/.\"\n",
    "    # !cp -r \"/content/drive/MyDrive/Master Thesis/01 - Datasets/DIV2K_800_5.zip\" \"Datasets/DIV2K_800_5.zip\"\n",
    "    !cp -r \"/content/drive/MyDrive/Master Thesis/01 - Datasets/DIV2K_5_fix.zip\" \"Datasets/DIV2K_5_fix.zip\"\n",
    "\n",
    "    # Extract datasets zips\n",
    "    for zip in os.listdir(\"Datasets\"):\n",
    "        with zipfile.ZipFile(os.path.join(\"Datasets\", zip), 'r') as zip_ref:\n",
    "            zip_ref.extractall(os.path.join(\"Datasets\", zip.replace(\".zip\", \"\")))\n",
    "            os.remove(os.path.join(\"Datasets\", zip))\n",
    "\n",
    "# Set relevant environment variables\n",
    "if is_using_colab():\n",
    "    device = \"cuda\"\n",
    "    experiments_dir = \"Experiments\"\n",
    "    datasets_path = r\"Datasets/\"\n",
    "else:\n",
    "    device = \"cuda\"\n",
    "    experiments_dir = \"../05 - Experiments\"\n",
    "    datasets_path = r\"C:\\datasets\"\n",
    "\n",
    "# Import required libraries\n",
    "import os\n",
    "import time\n",
    "import gc\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display\n",
    "\n",
    "from media_utils import plot_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets Definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PARAMS = {\n",
    "    \"limit\": None,\n",
    "    \"preload_to_RAM\": True,\n",
    "    \"crop_size\": (128, 128)\n",
    "}\n",
    "\n",
    "DATALOADER_PARAMS = {\n",
    "    \"batch_size\": 8,\n",
    "    \"num_workers\": 0,\n",
    "    \"pin_memory\": True,\n",
    "}\n",
    "\n",
    "if is_using_colab():\n",
    "    DATASET_PARAMS[\"preload_to_RAM\"] = True\n",
    "    DATALOADER_PARAMS[\"batch_size\"] = 64\n",
    "    DATALOADER_PARAMS[\"num_workers\"] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CED Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CED_DATASET_PATH = os.path.join(datasets_path, \"CEDDataset\")\n",
    "available_sequences = os.listdir(CED_DATASET_PATH)\n",
    "print(\"Available sequences:\")\n",
    "for i, seq in enumerate(available_sequences):\n",
    "    print(f\"{i:<5}{seq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import CEDDataset\n",
    "train_datasets_names = [\"simple_color_keyboard_1\", \"simple_fruit\"]\n",
    "train_dataset = CEDDataset(CED_DATASET_PATH, sequences=train_datasets_names, ignore_input_image=True, **DATASET_PARAMS)\n",
    "\n",
    "valid_datasets_names = [\"simple_rabbits\"]\n",
    "valid_dataset = CEDDataset(CED_DATASET_PATH, sequences=valid_datasets_names, ignore_input_image=True, **DATASET_PARAMS)\n",
    "\n",
    "test_datasets_names = [\"simple_color_keyboard_2\", \"simple_jenga_1\", \"simple_wires_1\"]\n",
    "test_dataset = CEDDataset(CED_DATASET_PATH, sequences=test_datasets_names, ignore_input_image=True, **DATASET_PARAMS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic Dataset from DIV2K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import DIV2KDataset\n",
    "DIV2K_DATASET_PATH = os.path.join(datasets_path, \"DIV2K_5_fix\")\n",
    "\n",
    "train_datasets_names = [\"{:04}\".format(i) for i in range(1, 800 + 1)]\n",
    "if TEST_MODE:\n",
    "    train_datasets_names = [\"{:04}\".format(i) for i in range(1, 25 + 1)]\n",
    "train_dataset = DIV2KDataset(DIV2K_DATASET_PATH, sequences=train_datasets_names, **DATASET_PARAMS)\n",
    "\n",
    "valid_datasets_names = [\"{:04}\".format(i) for i in range(801, 900 + 1)]\n",
    "if TEST_MODE:\n",
    "    valid_datasets_names = [\"{:04}\".format(i) for i in range(801, 805 + 1)]\n",
    "valid_dataset = DIV2KDataset(DIV2K_DATASET_PATH, sequences=valid_datasets_names, **DATASET_PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch_from_files(paths, load_batch_fn, pre_process_fn):\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for path in paths:\n",
    "\n",
    "        # If the path doesn't exist try to search .pt or .npz files        \n",
    "        for ext in [\".pt\", \".npz\"]:\n",
    "            if os.path.exists(path + ext):\n",
    "                path = path + ext\n",
    "                break\n",
    "\n",
    "        sample = pre_process_fn(load_batch_fn(path))\n",
    "        X.append(torch.tensor(sample[0], dtype=torch.float32))\n",
    "        y.append(torch.tensor(sample[1], dtype=torch.float32))\n",
    "    batch = torch.stack(X), torch.stack(y)\n",
    "    return batch\n",
    "\n",
    "\n",
    "train_batch_paths = [\n",
    "    os.path.join(DIV2K_DATASET_PATH, \"0001\", \"batch_0000\"),\n",
    "    os.path.join(DIV2K_DATASET_PATH, \"0002\", \"batch_0000\"),\n",
    "    os.path.join(DIV2K_DATASET_PATH, \"0003\", \"batch_0000\"),\n",
    "    os.path.join(DIV2K_DATASET_PATH, \"0004\", \"batch_0000\"),\n",
    "    os.path.join(DIV2K_DATASET_PATH, \"0005\", \"batch_0000\"),\n",
    "]\n",
    "valid_batch_paths = [\n",
    "    os.path.join(DIV2K_DATASET_PATH, \"0801\", \"batch_0000\"),\n",
    "    os.path.join(DIV2K_DATASET_PATH, \"0802\", \"batch_0000\"),\n",
    "    os.path.join(DIV2K_DATASET_PATH, \"0803\", \"batch_0000\"),\n",
    "    os.path.join(DIV2K_DATASET_PATH, \"0804\", \"batch_0000\"),\n",
    "    os.path.join(DIV2K_DATASET_PATH, \"0805\", \"batch_0000\"),\n",
    "]\n",
    "\n",
    "train_batch = generate_batch_from_files(train_batch_paths, train_dataset._load_batch, train_dataset.pre_process)\n",
    "valid_batch = generate_batch_from_files(valid_batch_paths, valid_dataset._load_batch, valid_dataset.pre_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 125 \t Train batches: 16        \n",
      "Valid samples: 25 \t Valid batches: 4         \n"
     ]
    }
   ],
   "source": [
    "from dataset import CEDDataset, ConcatBatchSampler\n",
    "\n",
    "# Check that there is no instersection between train and valid dataset\n",
    "union = set(train_datasets_names).union(set(valid_datasets_names))\n",
    "sum_of_lengths = sum([len(ds) for ds in [train_datasets_names, valid_datasets_names]])\n",
    "assert len(union) == sum_of_lengths, \"Some datasets are in common\"\n",
    "\n",
    "# Do not batch events coming from different datasets\n",
    "# samplers = [torch.utils.data.RandomSampler(ds) for ds in train_datasets]\n",
    "# sampler = ConcatBatchSampler(samplers=samplers, batch_size=TRAIN_DS_PARAMS[\"batch_size\"], drop_last=False)\n",
    "# train_dataloader = torch.utils.data.DataLoader(\n",
    "#     concat_ds, batch_sampler=sampler,\n",
    "# )\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, shuffle=True, **DATALOADER_PARAMS)\n",
    "valid_dataloader = torch.utils.data.DataLoader(valid_dataset, shuffle=False, **DATALOADER_PARAMS)\n",
    "\n",
    "DATASET_PARAMS.update({\n",
    "    \"train_datasets_names\": train_datasets_names, \n",
    "    \"valid_datasets_names\": valid_datasets_names, \n",
    "})\n",
    "\n",
    "print(\"Train samples: {} \\t Train batches: {:<10}\".format(len(train_dataset), len(train_dataloader)))\n",
    "print(\"Valid samples: {} \\t Valid batches: {:<10}\".format(len(valid_dataset), len(valid_dataloader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 134.89it/s]\n"
     ]
    }
   ],
   "source": [
    "# Inspect train dataloader\n",
    "from media_utils import save_video_tensors\n",
    "frames = []\n",
    "\n",
    "# Old dataset format\n",
    "# for (i, e), o in tqdm(train_dataloader):\n",
    "\n",
    "for e, o in tqdm(train_dataloader):\n",
    "  for batch in o:\n",
    "    frames.append(batch)\n",
    "\n",
    "save_video_tensors(\"train_frames.mp4\", frames, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 363.31it/s]\n"
     ]
    }
   ],
   "source": [
    "# Inspect valid dataloader\n",
    "from media_utils import save_video_tensors\n",
    "frames = []\n",
    "\n",
    "# Old dataset format\n",
    "# for (i, e), o in tqdm(valid_dataloader):\n",
    "\n",
    "for e, o in tqdm(valid_dataloader):\n",
    "  for batch in o:\n",
    "    frames.append(batch)\n",
    "\n",
    "save_video_tensors(\"valid_frames.mp4\", frames, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNet AutoEncoder Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "from models import EventsToImagesUNet\n",
    "\n",
    "PARAMS = {\n",
    "    \"input_channels\": 3 + 10,\n",
    "}\n",
    "\n",
    "model = EventsToImagesUNet(PARAMS[\"input_channels\"])\n",
    "# summary(model, input_size=(4, input_channels, 256, 336), device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import train_unet\n",
    "\n",
    "PARAMS.update({\n",
    "    \"n_epochs\": 3,\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"comment\": \"\",\n",
    "})\n",
    "PARAMS.update({\"train_dataset_params\": DATASET_PARAMS})\n",
    "\n",
    "train_unet(model, device, train_dataloader, PARAMS, log_path=experiments_dir, valid_ds=valid_dataloader, save_best_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model_thesis.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save input frames as video\n",
    "from media_utils import save_video_tensors\n",
    "imgs = []\n",
    "for batch in tqdm(train_dataloader):\n",
    "    (input_images, events_tensors), ground_truth_images = batch\n",
    "    imgs += input_images\n",
    "save_video_tensors(\"input_video.mp4\", imgs, 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save generated frames as video\n",
    "use_prev_images_as_input = False\n",
    "\n",
    "imgs = []\n",
    "for i, batch in enumerate(tqdm(train_dataloader)):\n",
    "    if use_prev_images_as_input and i != 0:\n",
    "        input_images = generated_images\n",
    "    else:\n",
    "        (input_images, events_tensors), ground_truth_images = batch\n",
    "    input_images = torch.einsum(\"bhwc -> bchw\", input_images)\n",
    "\n",
    "    input_tensors = torch.hstack((input_images, events_tensors))\n",
    "    input_tensors = input_tensors.to(device)\n",
    "\n",
    "    generated_images = model(input_tensors).cpu().detach()\n",
    "    generated_images = torch.einsum(\"bchw -> bhwc\", generated_images)\n",
    "\n",
    "    imgs += generated_images\n",
    "\n",
    "save_video_tensors(\"generated_video.mp4\", imgs, 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BS = 1; Seconds for one epoch: 142.36227083206177\n",
    "# BS = 2; Seconds for one epoch: 132.93124723434448\n",
    "# BS = 4; Seconds for one epoch: 127.95218682289124\n",
    "# BS = 8; Seconds for one epoch: 138.49955368041992\n",
    "# BS = 16; Seconds for one epoch: after 180 seconds it was at 3/10 batches so I stopped it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import TransformerModel\n",
    "\n",
    "PARAMS = {\n",
    "    \"input_shape\": (336, 256, 3),\n",
    "    \"encoding_size\": 512,\n",
    "    \"heads\": 4,\n",
    "    \"layers_number\": 2,\n",
    "}\n",
    "\n",
    "model = TransformerModel(PARAMS[\"input_shape\"], PARAMS[\"encoding_size\"], PARAMS[\"heads\"], PARAMS[\"layers_number\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import train_transformer\n",
    "\n",
    "PARAMS.update({\n",
    "    \"n_epochs\": 3,\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"comment\": \"\",\n",
    "})\n",
    "PARAMS.update({\"train_dataset_params\": DATASET_PARAMS})\n",
    "\n",
    "train_transformer(model, device, train_dataloader, PARAMS, log_path=experiments_dir, valid_ds=valid_dataloader, save_best_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "torch.save(model.state_dict(), \"transformer.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model\n",
    "model.load_state_dict(torch.load(\"transformer.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.autoencoder import EventEncoder, EventDecoder, EventAutoEncoder\n",
    "from torchinfo import summary\n",
    "\n",
    "PARAMS = {\n",
    "    \"n_filters\": [16, 16, 32, 32],\n",
    "    \"input_features\": 1,\n",
    "    \"lr\": 0.001\n",
    "}\n",
    "\n",
    "ee = EventEncoder(PARAMS[\"input_features\"], PARAMS[\"n_filters\"])\n",
    "ed = EventDecoder(PARAMS[\"n_filters\"][-1], PARAMS[\"n_filters\"][::-1])\n",
    "model = EventAutoEncoder(ee, ed, PARAMS[\"lr\"])\n",
    "\n",
    "# summary(eae, input_size=(8, 1, 128, 128), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_events = train_batch[0][:, 1]\n",
    "train_batch = (train_events.unsqueeze(1), train_events)\n",
    "valid_events = valid_batch[0][:, 1]\n",
    "valid_batch = (valid_events.unsqueeze(1), valid_events)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Event Encoder Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import EventEncoderTransformer\n",
    "from torchinfo import summary\n",
    "\n",
    "PARAMS = {\n",
    "    \"output_shape\": (336, 256, 3),\n",
    "    \"encoding_size\": 336,\n",
    "    \"heads\": 4,\n",
    "    \"layers_number\": 6\n",
    "}\n",
    "\n",
    "# The model is going to use the previous AutoEncoder, so be sure to use a pre-trained version of it\n",
    "\n",
    "model = EventEncoderTransformer(\n",
    "    output_shape=PARAMS[\"output_shape\"], \n",
    "    encoder=eae.encoder, \n",
    "    encoding_size=PARAMS[\"encoding_size\"], \n",
    "    heads=PARAMS[\"heads\"], \n",
    "    layers_number=PARAMS[\"layers_number\"]\n",
    ")\n",
    "\n",
    "# summary(model, input_size=(4, 10, 256, 336))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import train_transformer\n",
    "\n",
    "PARAMS.update({\n",
    "    \"n_epochs\": 300,\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"comment\": \"\"\n",
    "})\n",
    "PARAMS.update({\"train_dataset_params\": DATASET_PARAMS})\n",
    "\n",
    "train_transformer(model, device, train_dataloader, PARAMS, log_path=experiments_dir, valid_ds=valid_dataloader, save_best_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "torch.save(model.state_dict(), \"model_tae.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model.load_state_dict(torch.load(\"model_tae.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ViT-like Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean VRAM\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from models.transformer import VisionTransformer\n",
    "\n",
    "PARAMS = {\n",
    "    \"input_shape\": (10, 128, 128),\n",
    "    \"patch_size\": (32, 32),\n",
    "    \"encoding_size\": 256,\n",
    "    \"heads\": 4,\n",
    "    \"layers_number\": 3,\n",
    "    \"use_linear_proj\": True,\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"use_LPIPS\": False,\n",
    "    \"vgg_layer\": \"features.35\",\n",
    "}\n",
    "model = VisionTransformer(**PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_using_colab():\n",
    "    from IPython import get_ipython\n",
    "    get_ipython().magic('tensorboard --logdir \"Experiments/lightning_logs\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import LogImagesCallback, KerasProgressBar\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "PARAMS.update({\n",
    "    \"n_epochs\": 2,\n",
    "    \"comment\": \"Test with VGG54\"\n",
    "})\n",
    "PARAMS.update({\"train_dataset_params\": DATASET_PARAMS})\n",
    "\n",
    "callbacks = []\n",
    "\n",
    "train_batch = next(iter(train_dataloader))\n",
    "valid_batch = next(iter(valid_dataloader))\n",
    "callbacks.append(LogImagesCallback(train_batch, valid_batch, n=5, n_epochs=5))\n",
    "\n",
    "callbacks.append(KerasProgressBar())\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(monitor=\"val_loss\", mode=\"min\", save_last=True)\n",
    "checkpoint_callback.CHECKPOINT_NAME_LAST = \"{epoch}-last\"\n",
    "callbacks.append(checkpoint_callback)\n",
    "\n",
    "logger = pl.loggers.TensorBoardLogger(experiments_dir, version=PARAMS[\"comment\"])\n",
    "log_every = 50\n",
    "if len(train_dataloader) < log_every:\n",
    "    log_every = 1\n",
    "\n",
    "params_markdown = \"```json\\n\" + json.dumps(PARAMS, indent=2).replace(\"\\n\",\"  \\n\") + \"\\n```\"\n",
    "logger.experiment.add_text(\"params\", params_markdown)\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=PARAMS[\"n_epochs\"], callbacks=callbacks, accelerator=\"gpu\", profiler=None, logger=logger, log_every_n_steps=log_every)\n",
    "trainer.fit(model, train_dataloader, valid_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "torch.save(model.state_dict(), \"model_vit.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model.load_state_dict(torch.load(\"model_vit.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conv ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean VRAM\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from models.transformer import VisionTransformerConv\n",
    "\n",
    "PARAMS = {\n",
    "    \"input_shape\": (10, 128, 128),\n",
    "    \"patch_size\": (32, 32),\n",
    "    \"heads\": 4,\n",
    "    \"layers_number\": 1,\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"image_loss_weight\": 1,\n",
    "    \"feature_loss_weight\": 1e-2,\n",
    "}\n",
    "model = VisionTransformerConv(**PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "summary(model, input_size=(8, 10, 128, 128), col_names=[\"input_size\", \"output_size\", \"num_params\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.cnn import BasicCNN\n",
    "\n",
    "PARAMS = {\n",
    "    \"lr\": 0.001\n",
    "}\n",
    "model = BasicCNN(**PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "summary(model, input_size=(8, 10, 128, 128), col_names=[\"input_size\", \"output_size\", \"num_params\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Lightning Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import LogImagesCallback, KerasProgressBar, ColabSaveCallback\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "PARAMS.update({\n",
    "    \"n_epochs\": 50,\n",
    "    \"comment\": \"Small - little test\"\n",
    "})\n",
    "PARAMS.update({\"dataset_params\": DATASET_PARAMS})\n",
    "PARAMS.update({\"dataloader_params\": DATALOADER_PARAMS})\n",
    "\n",
    "\n",
    "callbacks = []\n",
    "callbacks.append(LogImagesCallback(train_batch, valid_batch, n=5, n_epochs=5))\n",
    "callbacks.append(KerasProgressBar())\n",
    "if is_using_colab():\n",
    "    dst_path = \"/content/drive/MyDrive/Master Thesis\"\n",
    "    colab_cb = ColabSaveCallback(\"Exp.zip\", dst_path, 60*60, [\"zip -r Exp.zip Experiments\"])\n",
    "    callbacks.append(colab_cb)\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(monitor=\"val_loss\", mode=\"min\", save_last=True)\n",
    "checkpoint_callback.CHECKPOINT_NAME_LAST = \"{epoch}-last\"\n",
    "callbacks.append(checkpoint_callback)\n",
    "\n",
    "logger = pl.loggers.TensorBoardLogger(experiments_dir, version=PARAMS[\"comment\"])\n",
    "log_every = 50\n",
    "if len(train_dataloader) < log_every:\n",
    "    log_every = 1\n",
    "\n",
    "params_markdown = \"```json\\n\" + json.dumps(PARAMS, indent=2).replace(\"\\n\",\"  \\n\") + \"\\n```\"\n",
    "logger.experiment.add_text(\"params\", params_markdown)\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=PARAMS[\"n_epochs\"], callbacks=callbacks, accelerator=\"gpu\", profiler=None, logger=logger, log_every_n_steps=log_every)\n",
    "trainer.fit(model, train_dataloader, valid_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.transformer import VisionTransformerConv\n",
    "checkpoint_path = r\"E:\\Cartelle Personali\\Fabrizio\\Universita\\Magistrale\\Tesi\\05 - Experiments\\lightning_logs\\Large, Long - 1 il, 1e-2 fl, bn relu, maxpool\\checkpoints\\epoch=243-step=15372.ckpt\"\n",
    "model = VisionTransformerConv.load_from_checkpoint(checkpoint_path, feature_loss_weight=None, image_loss_weight=None, map_location=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import LogImagesCallback\n",
    "\n",
    "def fig_to_numpy(fig):\n",
    "    canvas = fig.canvas \n",
    "    canvas.draw()\n",
    "    rgba = np.asarray(canvas.buffer_rgba())\n",
    "    return rgba\n",
    "\n",
    "def create_figure(batch, model):\n",
    "    cb = LogImagesCallback(train_batch, valid_batch)\n",
    "    x, y = batch\n",
    "    outputs = model(x)[0]\n",
    "    figs = []\n",
    "    for i in range(len(x)):\n",
    "        fig = cb._create_plot(outputs[i], y[i])\n",
    "        figs.append(fig_to_numpy(fig))\n",
    "        plt.close(fig)\n",
    "    return np.hstack(figs)\n",
    "\n",
    "path_split = checkpoint_path.split(os.sep)\n",
    "RUN_NAME = path_split[path_split.index(\"checkpoints\") - 1]\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.axis(\"off\")\n",
    "train_fig = create_figure(train_batch, model)\n",
    "plt.title(f\"{RUN_NAME} | Train\")\n",
    "plt.imsave(f\"{RUN_NAME}_train.png\", train_fig)\n",
    "plt.imshow(train_fig)\n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.axis(\"off\")\n",
    "plt.title(f\"{RUN_NAME} | Valid\")\n",
    "valid_fig = create_figure(valid_batch, model)\n",
    "plt.imsave(f\"{RUN_NAME}_valid.png\", valid_fig)\n",
    "plt.imshow(valid_fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save video results on train and test set\n",
    "from media_utils import save_predicted_video\n",
    "save_predicted_video(model, train_dataloader, \"train_prediction.mp4\")\n",
    "save_predicted_video(model, valid_dataloader, \"test_prediction.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_utils import save_events_frames_view\n",
    "save_events_frames_view(\"train_inspect.mp4\", train_dataset, model=model, fps=15, denorm=True)\n",
    "save_events_frames_view(\"test_inspect.mp4\", valid_dataset, model=model, fps=15, denorm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from media_utils import plot_square, predict_n_images\n",
    "\n",
    "N_EVAL_IMAGES = 16\n",
    "\n",
    "results = predict_n_images(train_dataset, N_EVAL_IMAGES, model)\n",
    "plot_square(results, size=3)\n",
    "\n",
    "results = predict_n_images(valid_dataset, N_EVAL_IMAGES, model)\n",
    "plot_square(results, size=3)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3.10.5 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f60749a70c461b41520daeabcfcd14a2d1bc9b0f5ff8acdc44832defeb641db7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
